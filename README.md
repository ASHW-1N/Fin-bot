# Fin-bot
# ğŸ“ˆ AI Finance Assistant ğŸ¤–ğŸ’°

An intelligent, real-time financial question-answering assistant built using **LLaMA 2** and **Retrieval-Augmented Generation (RAG)**. This project blends the power of **open-source large language models** with **real-world financial news** to deliver smart, context-aware insights in natural human language â€” without hallucination or fluff.

> Think of it as ChatGPT for financial news, but fully local and under your control. ğŸ’»ğŸ”

---

## ğŸ” What It Does

When you ask a financial question like:

> *"How did Asian markets perform today?"*

The assistant will:

1. Retrieve **relevant news snippets** from a local database of articles.
2. Use those snippets to **build an intelligent context**.
3. Feed that context into a local **LLaMA 2 model** using a smart prompt.
4. Return a **concise, well-reasoned answer** based on actual financial news.

Itâ€™s Retrieval-Augmented Generation done right â€” fast, grounded, and transparent.

---

## ğŸš€ Features

### âœ… Retrieval-Augmented Generation (RAG)
- Dynamically retrieves **top relevant news** articles using **FAISS vector similarity search**.
- Embeds queries and documents using **`sentence-transformers`** (`all-MiniLM-L6-v2`).

### ğŸ§  LLaMA 2 Integration (Offline, Local)
- Powered by Meta's **LLaMA 2 7B Chat** model (quantized `.gguf`).
- Fully local via [`llama.cpp`](https://github.com/ggerganov/llama.cpp) Python bindings.
- Custom prompt template ensures **factual**, **honest**, and **non-hallucinated** responses.

### ğŸ“š Real-Time Financial Knowledge
- Loads news from a local `news.json` file (can be automated with RSS or APIs).
- Formats news into concise summaries for improved grounding.

### ğŸ–¥ï¸ Built with Streamlit
- Clean, minimal, and interactive UI.
- Type in your query and get a real-time AI response with retrieved context.

### ğŸ’¡ Thoughtful Prompting
- Uses advanced prompt engineering techniques:
  - Adds clear system role: â€œYou are a financial assistant...â€
  - Includes fallback logic: â€œIf data is missing, say so.â€
  - Avoids bluffing or hallucinating future events.

---

## ğŸ› ï¸ Tech Stack

| Layer         | Tool/Library              | Purpose                                 |
|---------------|---------------------------|-----------------------------------------|
| ğŸ’¬ LLM        | `llama.cpp` (LLaMA 2 Chat)| Natural Language Response Generation    |
| ğŸ“„ Embeddings | `sentence-transformers`   | Semantic embedding for RAG              |
| ğŸ” Search     | `faiss`                   | Fast approximate nearest neighbors search|
| ğŸŒ UI         | `Streamlit`               | Interactive frontend for user queries   |
| ğŸ“¦ Data       | JSON (news.json)          | Financial article context store         |

---

## ğŸ“‚ Project Structure

inance-assistant/
â”œâ”€â”€ app.py # Streamlit app entry point
â”œâ”€â”€ llama_interface.py # Handles LLaMA prompting logic
â”œâ”€â”€ financial_news.py # FAISS-based news retrieval
â”œâ”€â”€ news.json # Source news articles
â”œâ”€â”€ models/ # Contains LLaMA GGUF model
â”‚ â””â”€â”€ llama-2-7b-chat.Q8_0.gguf
â”œâ”€â”€ README.md # This file
â””â”€â”€ requirements.txt # Dependencies


---

## ğŸ“Œ Example Queries You Can Try

- â€œTop stocks to watch next week?â€
- â€œWhatâ€™s driving gold prices today?â€
- â€œHow did European markets react to the Fed announcement?â€
- â€œAny news on Indian tech sector performance?â€
- â€œHas there been a rise in crude oil prices?â€

---

## ğŸ”’ Privacy & Local Execution

This project is **fully offline**:
- No API calls
- No cloud services
- No user data shared

Perfect for **local research**, **personal finance analysis**, or **air-gapped environments**.

---

## ğŸ§  How It Works â€“ Under the Hood

### Step-by-step Flow:
1. ğŸ“° **Load Articles** â†’ From `news.json`.
2. ğŸ” **Vector Indexing** â†’ Create dense vectors with `sentence-transformers`, indexed via `faiss`.
3. âœï¸ **User Query** â†’ Entered in the Streamlit UI.
4. ğŸ” **Retrieve Top Matches** â†’ FAISS finds top-k most relevant articles.
5. ğŸ§  **LLaMA Prompting** â†’ Query + context fed into `llama.cpp` model.
6. ğŸ“¤ **Answer Returned** â†’ Displayed in natural-sounding text.

---

## ğŸ“¥ Installation

```bash
git clone https://github.com/your-username/finance-assistant
cd finance-assistant

# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Place your GGUF model in /models/
# you can download the Llama quantized Model from --https://huggingface.co/TheBloke/finance-LLM-GGUF
# and place it in your /models folder

# Run the app
streamlit run app.py


